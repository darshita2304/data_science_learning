{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = pd.read_csv(\"Forwards.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>India Id</th>\n",
       "      <th>Trade Id</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Trade Date</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>MR_MATURITY_DATE</th>\n",
       "      <th>Entered Date</th>\n",
       "      <th>CounterParty (F)</th>\n",
       "      <th>Book</th>\n",
       "      <th>Buy/Sell</th>\n",
       "      <th>Currency Pair</th>\n",
       "      <th>Prim Cur</th>\n",
       "      <th>Prim Amt</th>\n",
       "      <th>Trade Price</th>\n",
       "      <th>Sec Cur</th>\n",
       "      <th>Sec Amt</th>\n",
       "      <th>TradeStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26997</td>\n",
       "      <td>13782015.0</td>\n",
       "      <td>2199667/FXFlexiForward/MerchantFX/RETAIL/USD/I...</td>\n",
       "      <td>01-03-21 0:00</td>\n",
       "      <td>01-09-21</td>\n",
       "      <td>07-03-24</td>\n",
       "      <td>01-03-21 0:00</td>\n",
       "      <td>Counterparty 1</td>\n",
       "      <td>TB.IND.TRADING.TIMEOPT</td>\n",
       "      <td>Buy</td>\n",
       "      <td>USD/INR</td>\n",
       "      <td>USD</td>\n",
       "      <td>450000</td>\n",
       "      <td>75.38</td>\n",
       "      <td>INR</td>\n",
       "      <td>-33921000.00</td>\n",
       "      <td>VERIFIED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27966</td>\n",
       "      <td>14734932.0</td>\n",
       "      <td>2219298/FXFlexiForward/MerchantFX/RETAIL/USD/I...</td>\n",
       "      <td>12-03-21 0:00</td>\n",
       "      <td>30-07-21</td>\n",
       "      <td>23-03-25</td>\n",
       "      <td>12-03-21 0:00</td>\n",
       "      <td>Counterparty 2</td>\n",
       "      <td>TB.IND.TRADING.TIMEOPT</td>\n",
       "      <td>Buy</td>\n",
       "      <td>USD/INR</td>\n",
       "      <td>USD</td>\n",
       "      <td>83395.63</td>\n",
       "      <td>74.21</td>\n",
       "      <td>INR</td>\n",
       "      <td>-6188581.22</td>\n",
       "      <td>VERIFIED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   India Id    Trade Id                                Product Description  \\\n",
       "0     26997  13782015.0  2199667/FXFlexiForward/MerchantFX/RETAIL/USD/I...   \n",
       "1     27966  14734932.0  2219298/FXFlexiForward/MerchantFX/RETAIL/USD/I...   \n",
       "\n",
       "      Trade Date Start Date MR_MATURITY_DATE   Entered Date CounterParty (F)  \\\n",
       "0  01-03-21 0:00   01-09-21         07-03-24  01-03-21 0:00   Counterparty 1   \n",
       "1  12-03-21 0:00   30-07-21         23-03-25  12-03-21 0:00   Counterparty 2   \n",
       "\n",
       "                     Book Buy/Sell Currency Pair Prim Cur  Prim Amt  \\\n",
       "0  TB.IND.TRADING.TIMEOPT      Buy       USD/INR      USD    450000   \n",
       "1  TB.IND.TRADING.TIMEOPT      Buy       USD/INR      USD  83395.63   \n",
       "\n",
       "  Trade Price Sec Cur      Sec Amt TradeStatus  \n",
       "0       75.38     INR -33921000.00    VERIFIED  \n",
       "1       74.21     INR  -6188581.22    VERIFIED  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'dict'>\n",
      "{'missing_columns_source': [], 'is_pkduplicated': True, 'is_pknull': True, 'primary_key_dict': {'India Id': {'duplicated': [], 'null': []}, 'Trade Id': {'duplicated': [1, 8, 13, 24, 26], 'null': [14]}, 'Product Description': {'duplicated': [4, 20], 'null': [4, 20]}, 'CounterParty (F)': {'duplicated': [], 'null': []}}, 'datatype_missmatch': {'India Id': {'both_same': True, 'datatypes': {'identified_data_type': 'integer', 'all_same_datatype': True, 'null_index': [], 'integer_details': {'int_index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26], 'int_nagative_index': [], 'int_zero_index': []}, 'string_index': [], 'decimal_details': {'decimal_index': [], 'decimal_format': ''}, 'date_details': {'date_index': {}, 'date_format': []}}}, 'Trade Id': {'both_same': False, 'datatypes': {'identified_data_type': 'integer', 'all_same_datatype': False, 'null_index': [14], 'integer_details': {'int_index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26], 'int_nagative_index': [9], 'int_zero_index': []}, 'string_index': [], 'decimal_details': {'decimal_index': [], 'decimal_format': ''}, 'date_details': {'date_index': {}, 'date_format': []}}}, 'Product Description': {'both_same': False, 'datatypes': {'identified_data_type': 'string', 'all_same_datatype': False, 'null_index': [4, 20], 'integer_details': {'int_index': [10, 14], 'int_nagative_index': [], 'int_zero_index': [10]}, 'string_index': [0, 1, 2, 3, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26], 'decimal_details': {'decimal_index': [], 'decimal_format': ''}, 'date_details': {'date_index': {'21': ['Date (dd-mm-yy hh:mm)']}, 'date_format': ['Date (dd-mm-yy hh:mm)']}}}, 'Trade Date': {'both_same': False, 'datatypes': {'identified_data_type': \"Date (['Date (dd-mm-yy hh:mm)'])\", 'all_same_datatype': False, 'null_index': [15], 'integer_details': {'int_index': [], 'int_nagative_index': [], 'int_zero_index': []}, 'string_index': [19, 23], 'decimal_details': {'decimal_index': [], 'decimal_format': ''}, 'date_details': {'date_index': {'0': ['Date (dd-mm-yy hh:mm)'], '1': ['Date (dd-mm-yy hh:mm)'], '2': ['Date (dd-mm-yy hh:mm)'], '3': ['Date (dd-mm-yy hh:mm)'], '4': ['Date (dd-mm-yy hh:mm)'], '5': ['Date (dd-mm-yy hh:mm)'], '6': ['Date (dd-mm-yy hh:mm)'], '7': ['Date (dd-mm-yy hh:mm)'], '8': ['Date (dd-mm-yy hh:mm)'], '9': ['Date (dd-mm-yy hh:mm)'], '10': ['Date (dd-mm-yy hh:mm)'], '11': ['Date (dd-mm-yy hh:mm)'], '12': ['Date (dd-mm-yy hh:mm)'], '13': ['Date (dd-mm-yy hh:mm)'], '14': ['Date (dd-mm-yy hh:mm)'], '16': ['Date (dd-mm-yy hh:mm)'], '17': ['Date (dd-mm-yy hh:mm)'], '18': ['Date (dd-mm-yy hh:mm)'], '20': ['Date (dd-mm-yy hh:mm)'], '21': ['Date (dd-mm-yy hh:mm)'], '22': ['Date (dd-mm-yy hh:mm)'], '24': ['Date (dd-mm-yy hh:mm)'], '25': ['Date (dd-mm-yy hh:mm)'], '26': ['Date (dd-mm-yy hh:mm)']}, 'date_format': ['Date (dd-mm-yy hh:mm)']}}}, 'Start Date': {'both_same': True, 'datatypes': {'identified_data_type': 'Date', 'all_same_datatype': True, 'null_index': [], 'integer_details': {'int_index': [], 'int_nagative_index': [], 'int_zero_index': []}, 'string_index': [], 'decimal_details': {'decimal_index': [], 'decimal_format': ''}, 'date_details': {'date_index': {'0': ['Date (yy-mm-dd)', 'Date (mm-dd-yy)'], '1': ['Date (yy-mm-dd)'], '2': ['Date (yy-mm-dd)'], '3': ['Date (yy-mm-dd)'], '4': ['Date (yy-mm-dd)'], '5': ['Date (yy-mm-dd)'], '6': ['Date (yy-mm-dd)', 'Date (mm-dd-yy)'], '7': ['Date (yy-mm-dd)'], '8': ['Date (yy-mm-dd)'], '9': ['Date (yy-mm-dd)'], '10': ['Date (yy-mm-dd)', 'Date (mm-dd-yy)'], '11': ['Date (yy-mm-dd)'], '12': ['Date (yy-mm-dd)'], '13': ['Date (yy-mm-dd)'], '14': ['Date (yy-mm-dd)'], '15': ['Date (yy-mm-dd)', 'Date (mm-dd-yy)'], '16': ['Date (yy-mm-dd)'], '17': ['Date (yy-mm-dd)'], '18': ['Date (yy-mm-dd)'], '19': ['Date (yy-mm-dd)'], '20': ['Date (yy-mm-dd)', 'Date (mm-dd-yy)'], '21': ['Date (yy-mm-dd)', 'Date (mm-dd-yy)'], '22': ['Date (yy-mm-dd)', 'Date (mm-dd-yy)'], '23': ['Date (yy-mm-dd)'], '24': ['Date (yy-mm-dd)'], '25': ['Date (yy-mm-dd)'], '26': ['Date (yy-mm-dd)']}, 'date_format': ['Date (yy-mm-dd)', 'Date (mm-dd-yy)']}}}, 'MR_MATURITY_DATE': {'both_same': True, 'datatypes': {'identified_data_type': 'Date', 'all_same_datatype': True, 'null_index': [], 'integer_details': {'int_index': [], 'int_nagative_index': [], 'int_zero_index': []}, 'string_index': [], 'decimal_details': {'decimal_index': [], 'decimal_format': ''}, 'date_details': {'date_index': {'0': ['Date (yy-mm-dd)', 'Date (mm-dd-yy)'], '1': ['Date (yy-mm-dd)'], '2': ['Date (yy-mm-dd)'], '3': ['Date (yy-mm-dd)'], '4': ['Date (yy-mm-dd)'], '5': ['Date (yy-mm-dd)'], '6': ['Date (yy-mm-dd)'], '7': ['Date (yy-mm-dd)'], '8': ['Date (yy-mm-dd)'], '9': ['Date (yy-mm-dd)'], '10': ['Date (yy-mm-dd)'], '11': ['Date (yy-mm-dd)'], '12': ['Date (yy-mm-dd)', 'Date (mm-dd-yy)'], '13': ['Date (yy-mm-dd)'], '14': ['Date (yy-mm-dd)', 'Date (mm-dd-yy)'], '15': ['Date (yy-mm-dd)', 'Date (mm-dd-yy)'], '16': ['Date (yy-mm-dd)'], '17': ['Date (yy-mm-dd)', 'Date (mm-dd-yy)'], '18': ['Date (yy-mm-dd)'], '19': ['Date (yy-mm-dd)'], '20': ['Date (yy-mm-dd)'], '21': ['Date (yy-mm-dd)', 'Date (mm-dd-yy)'], '22': ['Date (yy-mm-dd)'], '23': ['Date (yy-mm-dd)', 'Date (mm-dd-yy)'], '24': ['Date (yy-mm-dd)', 'Date (mm-dd-yy)'], '25': ['Date (yy-mm-dd)'], '26': ['Date (yy-mm-dd)', 'Date (mm-dd-yy)']}, 'date_format': ['Date (yy-mm-dd)', 'Date (mm-dd-yy)']}}}, 'Entered Date': {'both_same': False, 'datatypes': {'identified_data_type': \"Date (['Date (dd-mm-yy hh:mm)'])\", 'all_same_datatype': False, 'null_index': [], 'integer_details': {'int_index': [21], 'int_nagative_index': [], 'int_zero_index': []}, 'string_index': [], 'decimal_details': {'decimal_index': [], 'decimal_format': ''}, 'date_details': {'date_index': {'0': ['Date (dd-mm-yy hh:mm)'], '1': ['Date (dd-mm-yy hh:mm)'], '2': ['Date (dd-mm-yy hh:mm)'], '3': ['Date (dd-mm-yy hh:mm)'], '4': ['Date (dd-mm-yy hh:mm)'], '5': ['Date (dd-mm-yy hh:mm)'], '6': ['Date (dd-mm-yy hh:mm)'], '7': ['Date (dd-mm-yy hh:mm)'], '8': ['Date (dd-mm-yy hh:mm)'], '9': ['Date (dd-mm-yy hh:mm)'], '10': ['Date (dd-mm-yy hh:mm)'], '11': ['Date (dd-mm-yy hh:mm)'], '12': ['Date (dd-mm-yy hh:mm)'], '13': ['Date (dd-mm-yy hh:mm)'], '14': ['Date (dd-mm-yy hh:mm)'], '15': ['Date (dd-mm-yy hh:mm)'], '16': ['Date (dd-mm-yy hh:mm)'], '17': ['Date (dd-mm-yy hh:mm)'], '18': ['Date (dd-mm-yy hh:mm)'], '19': ['Date (dd-mm-yy hh:mm)'], '20': ['Date (dd-mm-yy hh:mm)'], '22': ['Date (dd-mm-yy hh:mm)'], '23': ['Date (dd-mm-yy hh:mm)'], '24': ['Date (dd-mm-yy hh:mm)'], '25': ['Date (dd-mm-yy hh:mm)'], '26': ['Date (dd-mm-yy hh:mm)']}, 'date_format': ['Date (dd-mm-yy hh:mm)']}}}, 'CounterParty (F)': {'both_same': True, 'datatypes': {'identified_data_type': 'string', 'all_same_datatype': True, 'null_index': [], 'integer_details': {'int_index': [], 'int_nagative_index': [], 'int_zero_index': []}, 'string_index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26], 'decimal_details': {'decimal_index': [], 'decimal_format': ''}, 'date_details': {'date_index': {}, 'date_format': []}}}, 'Book': {'both_same': True, 'datatypes': {'identified_data_type': 'string', 'all_same_datatype': True, 'null_index': [], 'integer_details': {'int_index': [], 'int_nagative_index': [], 'int_zero_index': []}, 'string_index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26], 'decimal_details': {'decimal_index': [], 'decimal_format': ''}, 'date_details': {'date_index': {}, 'date_format': []}}}, 'Buy/Sell': {'both_same': True, 'datatypes': {'identified_data_type': 'string', 'all_same_datatype': True, 'null_index': [], 'integer_details': {'int_index': [], 'int_nagative_index': [], 'int_zero_index': []}, 'string_index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26], 'decimal_details': {'decimal_index': [], 'decimal_format': ''}, 'date_details': {'date_index': {}, 'date_format': []}}}, 'Currency Pair': {'both_same': True, 'datatypes': {'identified_data_type': 'string', 'all_same_datatype': True, 'null_index': [], 'integer_details': {'int_index': [], 'int_nagative_index': [], 'int_zero_index': []}, 'string_index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26], 'decimal_details': {'decimal_index': [], 'decimal_format': ''}, 'date_details': {'date_index': {}, 'date_format': []}}}, 'Prim Cur': {'both_same': True, 'datatypes': {'identified_data_type': 'string', 'all_same_datatype': True, 'null_index': [], 'integer_details': {'int_index': [], 'int_nagative_index': [], 'int_zero_index': []}, 'string_index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26], 'decimal_details': {'decimal_index': [], 'decimal_format': ''}, 'date_details': {'date_index': {}, 'date_format': []}}}, 'Prim Amt': {'both_same': False, 'datatypes': {'identified_data_type': 'integer', 'all_same_datatype': False, 'null_index': [], 'integer_details': {'int_index': [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 24, 25, 26], 'int_nagative_index': [10, 11, 12, 13, 15, 17, 20, 22, 24, 25, 26], 'int_zero_index': [5, 9, 14, 16, 18, 19]}, 'string_index': [21], 'decimal_details': {'decimal_index': [1, 23], 'decimal_format': 'decimal(9,2)'}, 'date_details': {'date_index': {}, 'date_format': []}}}, 'Trade Price': {'both_same': False, 'datatypes': {'identified_data_type': 'decimal(4,2)', 'all_same_datatype': False, 'null_index': [3], 'integer_details': {'int_index': [], 'int_nagative_index': [], 'int_zero_index': []}, 'string_index': [22], 'decimal_details': {'decimal_index': [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26], 'decimal_format': 'decimal(4,2)'}, 'date_details': {'date_index': {}, 'date_format': []}}}, 'Sec Cur': {'both_same': False, 'datatypes': {'identified_data_type': 'string', 'all_same_datatype': False, 'null_index': [9], 'integer_details': {'int_index': [], 'int_nagative_index': [], 'int_zero_index': []}, 'string_index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26], 'decimal_details': {'decimal_index': [], 'decimal_format': ''}, 'date_details': {'date_index': {}, 'date_format': []}}}, 'Sec Amt': {'both_same': False, 'datatypes': {'identified_data_type': 'integer', 'all_same_datatype': False, 'null_index': [], 'integer_details': {'int_index': [0, 2, 3, 4, 5, 6, 7, 8, 11, 14, 15, 16, 19, 20, 21, 22, 25, 26], 'int_nagative_index': [0, 2, 3, 4, 6, 7, 8], 'int_zero_index': [5, 16, 19]}, 'string_index': [], 'decimal_details': {'decimal_index': [1, 9, 10, 12, 13, 17, 18, 23, 24], 'decimal_format': 'decimal(11,2)'}, 'date_details': {'date_index': {}, 'date_format': []}}}, 'TradeStatus': {'both_same': True, 'datatypes': {'identified_data_type': 'string', 'all_same_datatype': True, 'null_index': [], 'integer_details': {'int_index': [], 'int_nagative_index': [], 'int_zero_index': []}, 'string_index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26], 'decimal_details': {'decimal_index': [], 'decimal_format': ''}, 'date_details': {'date_index': {}, 'date_format': []}}}}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('sample.json') as json_file:\n",
    "    quality_check_obj = json.load(json_file)\n",
    "\n",
    "    # Print the type of data variable\n",
    "    print(\"Type:\", type(quality_check_obj))\n",
    "    \n",
    "print(quality_check_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_checks_excel(source_df, quality_check_obj):\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    \n",
    "    copy_source = source_df\n",
    "    missing_columns = quality_check_obj[\"missing_columns_source\"]\n",
    "    \n",
    "    # if not quality_checks_dict[\"MissingColumns\"][\"discard\"]: #include\n",
    "    #     for col in missing_columns:\n",
    "    #         copy_source[col +(\"(missing)\")] = \"\"\n",
    "    # else: # dicard columns\n",
    "    #     copy_source.drop(missing_columns)\n",
    "\n",
    "    copy_source[\"check_fail_errors\"] = \"\"\n",
    "    copy_source[\"discard_flag\"] = 0 # 0 -keep 1-discard\n",
    "    \n",
    "        \n",
    "    ## adding primarykey and duplicated missing check_fail_errors...    \n",
    "    if quality_check_obj[\"is_pkduplicated\"] and quality_check_obj[\"is_pknull\"]: \n",
    "        pk_dict = quality_check_obj[\"primary_key_dict\"]           \n",
    "        for col in pk_dict:            \n",
    "            if len(pk_dict[col][\"null\"]) > 0 : # index of null values\n",
    "                for i in pk_dict[col][\"null\"]:\n",
    "                    copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \": Primary key is Null ]\"\n",
    "                    \n",
    "            if len(pk_dict[col][\"duplicated\"]) > 0 : # index of duplicated value\n",
    "                for i in pk_dict[col][\"duplicated\"]: # i is cell_index\n",
    "                    copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \": Primary key is duplicated ]\"\n",
    "    \n",
    "    ## adding datatype missmatch check_fail_errors\n",
    "    dt_dict = quality_check_obj[\"datatype_missmatch\"]\n",
    "    \n",
    "    set2 = set(i for i in range(0, source_df.shape[0])) # all rows index\n",
    "    \n",
    "    for col in dt_dict:\n",
    "        if not dt_dict[col][\"both_same\"]: # if source and report columns not same\n",
    "            \n",
    "            datatype_obj = dt_dict[col][\"datatypes\"]\n",
    "            \n",
    "            if len(datatype_obj[\"null_index\"]): # any datatype null should be highlighted\n",
    "                for i in datatype_obj[\"null_index\"]:\n",
    "                    copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" Null value ]\"\n",
    "            \n",
    "            if not datatype_obj[\"all_same_datatype\"]: # if not same than hightlight it\n",
    "                \n",
    "                if datatype_obj[\"identified_data_type\"] == \"integer\": # majority int found                    \n",
    "                    \n",
    "                    int_details = datatype_obj[\"integer_details\"]\n",
    "                    # all integer include zero and -ve\n",
    "                    set_int = set(int_details[\"int_index\"]) #int list which not need to hightlight\n",
    "                    not_int_set = set2 - set_int # index which hv not int\n",
    "                    for i in not_int_set:\n",
    "                        copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" not Integer ]\"\n",
    "                    \n",
    "                    # all integer not zero\n",
    "                    set_zero = set(int_details[\"int_zero_index\"]) #int list which is 0\n",
    "                    for i in set_zero:\n",
    "                        copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" is Zero ]\"\n",
    "                        \n",
    "                    # all integer not -ve\n",
    "                    set_negative = set(int_details[\"int_nagative_index\"]) #int list which not need to hightlight\n",
    "                    for i in set_negative:\n",
    "                        copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" is Negative ]\"\n",
    "                        \n",
    "                elif datatype_obj[\"identified_data_type\"] == \"string\": # majority string found\n",
    "                    set_string = set(datatype_obj[\"string_index\"]) #string list which not need to hightlight\n",
    "                    not_string_set = set2 - set_string # index which hv not string..\n",
    "                    \n",
    "                    for i in not_string_set:\n",
    "                        copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" not String ]\"\n",
    "                        \n",
    "                elif \"decimal\" in datatype_obj[\"identified_data_type\"]:# majority decimal found\n",
    "                    decimal_detail = datatype_obj[\"decimal_details\"]\n",
    "                    set_decimal = set(decimal_detail[\"decimal_index\"]) #decimal list which not need to hightlight\n",
    "                    diff = set2 - set_decimal # index which hv not decimal\n",
    "                    for i in diff:\n",
    "                        copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" not decimal with \" + decimal_detail[\"decimal_format\"] + \" format]\"\n",
    "                \n",
    "                elif \"Date\" in datatype_obj[\"identified_data_type\"]:\n",
    "                    date_detail = datatype_obj[\"date_details\"]                    \n",
    "                    set_date = set(date_detail[\"date_index\"])\n",
    "                    \n",
    "                    diff = set2 - set_date # index which hv not decimal\n",
    "                    for i in diff:\n",
    "                        copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" not a date format with \" + str(date_detail[\"date_format\"]) + \" format]\"                             \n",
    "                             \n",
    "    print(\"*********** new excel witch check file records *******\")  \n",
    "    print(copy_source.head(2))\n",
    "    \n",
    "    copy_source.to_csv('sample.csv')\n",
    "    \n",
    "    \n",
    "    return True\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** new excel witch check file records *******\n",
      "   India Id    Trade Id                                Product Description  \\\n",
      "0     26997  13782015.0  2199667/FXFlexiForward/MerchantFX/RETAIL/USD/I...   \n",
      "1     27966  14734932.0  2219298/FXFlexiForward/MerchantFX/RETAIL/USD/I...   \n",
      "\n",
      "      Trade Date Start Date MR_MATURITY_DATE   Entered Date CounterParty (F)  \\\n",
      "0  01-03-21 0:00   01-09-21         07-03-24  01-03-21 0:00   Counterparty 1   \n",
      "1  12-03-21 0:00   30-07-21         23-03-25  12-03-21 0:00   Counterparty 2   \n",
      "\n",
      "                     Book Buy/Sell Currency Pair Prim Cur  Prim Amt  \\\n",
      "0  TB.IND.TRADING.TIMEOPT      Buy       USD/INR      USD    450000   \n",
      "1  TB.IND.TRADING.TIMEOPT      Buy       USD/INR      USD  83395.63   \n",
      "\n",
      "  Trade Price Sec Cur      Sec Amt TradeStatus  \\\n",
      "0       75.38     INR -33921000.00    VERIFIED   \n",
      "1       74.21     INR  -6188581.22    VERIFIED   \n",
      "\n",
      "                                   check_fail_errors  discard_flag  \n",
      "0  [Trade Date not a date format with ['Date (dd-...             0  \n",
      "1  [Trade Id: Primary key is duplicated ][Trade D...             0  \n"
     ]
    }
   ],
   "source": [
    "created = create_checks_excel(source_df, quality_check_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_checks_excel_summary(source_df, quality_check_obj):\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    \n",
    "    # copy_source = source_df\n",
    "    missing_columns = quality_check_obj[\"missing_columns_source\"]\n",
    "    \n",
    "    summary_df = pd.DataFrame()\n",
    "    \n",
    "    copy_source[\"check_fail_errors\"] = \"\"\n",
    "    copy_source[\"discard_flag\"] = 0 # 0 -keep 1-discard\n",
    "    \n",
    "        \n",
    "    ## adding primarykey and duplicated missing check_fail_errors...    \n",
    "    if quality_check_obj[\"is_pkduplicated\"] and quality_check_obj[\"is_pknull\"]: \n",
    "        pk_dict = quality_check_obj[\"primary_key_dict\"]           \n",
    "        for col in pk_dict:            \n",
    "            if len(pk_dict[col][\"null\"]) > 0 : # index of null values\n",
    "                for i in pk_dict[col][\"null\"]:\n",
    "                    copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \": Primary key is Null ]\"\n",
    "                    \n",
    "            if len(pk_dict[col][\"duplicated\"]) > 0 : # index of duplicated value\n",
    "                for i in pk_dict[col][\"duplicated\"]: # i is cell_index\n",
    "                    copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \": Primary key is duplicated ]\"\n",
    "    \n",
    "    ## adding datatype missmatch check_fail_errors\n",
    "    dt_dict = quality_check_obj[\"datatype_missmatch\"]\n",
    "    \n",
    "    set2 = set(i for i in range(0, source_df.shape[0])) # all rows index\n",
    "    \n",
    "    for col in dt_dict:\n",
    "        if not dt_dict[col][\"both_same\"]: # if source and report columns not same\n",
    "            \n",
    "            datatype_obj = dt_dict[col][\"datatypes\"]\n",
    "            \n",
    "            if len(datatype_obj[\"null_index\"]): # any datatype null should be highlighted\n",
    "                for i in datatype_obj[\"null_index\"]:\n",
    "                    copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" Null value ]\"\n",
    "            \n",
    "            if not datatype_obj[\"all_same_datatype\"]: # if not same than hightlight it\n",
    "                \n",
    "                if datatype_obj[\"identified_data_type\"] == \"integer\": # majority int found                    \n",
    "                    \n",
    "                    int_details = datatype_obj[\"integer_details\"]\n",
    "                    # all integer include zero and -ve\n",
    "                    set_int = set(int_details[\"int_index\"]) #int list which not need to hightlight\n",
    "                    not_int_set = set2 - set_int # index which hv not int\n",
    "                    for i in not_int_set:\n",
    "                        copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" not Integer ]\"\n",
    "                    \n",
    "                    # all integer not zero\n",
    "                    set_zero = set(int_details[\"int_zero_index\"]) #int list which is 0\n",
    "                    for i in set_zero:\n",
    "                        copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" is Zero ]\"\n",
    "                        \n",
    "                    # all integer not -ve\n",
    "                    set_negative = set(int_details[\"int_nagative_index\"]) #int list which not need to hightlight\n",
    "                    for i in set_negative:\n",
    "                        copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" is Negative ]\"\n",
    "                        \n",
    "                elif datatype_obj[\"identified_data_type\"] == \"string\": # majority string found\n",
    "                    set_string = set(datatype_obj[\"string_index\"]) #string list which not need to hightlight\n",
    "                    not_string_set = set2 - set_string # index which hv not string..\n",
    "                    \n",
    "                    for i in not_string_set:\n",
    "                        copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" not String ]\"\n",
    "                        \n",
    "                elif \"decimal\" in datatype_obj[\"identified_data_type\"]:# majority decimal found\n",
    "                    decimal_detail = datatype_obj[\"decimal_details\"]\n",
    "                    set_decimal = set(decimal_detail[\"decimal_index\"]) #decimal list which not need to hightlight\n",
    "                    diff = set2 - set_decimal # index which hv not decimal\n",
    "                    for i in diff:\n",
    "                        copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" not decimal with \" + decimal_detail[\"decimal_format\"] + \" format]\"\n",
    "                \n",
    "                elif \"Date\" in datatype_obj[\"identified_data_type\"]:\n",
    "                    date_detail = datatype_obj[\"date_details\"]                    \n",
    "                    set_date = set(date_detail[\"date_index\"])\n",
    "                    \n",
    "                    diff = set2 - set_date # index which hv not decimal\n",
    "                    for i in diff:\n",
    "                        copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" not a date format with \" + str(date_detail[\"date_format\"]) + \" format]\"                             \n",
    "                             \n",
    "    print(\"*********** new excel witch check file records *******\")  \n",
    "    print(copy_source.head(2))\n",
    "    \n",
    "    copy_source.to_csv('sample.csv')\n",
    "    \n",
    "    \n",
    "    return True\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "def value_to_excel(v):\n",
    "    return str(v).replace(\"[]\",\"\")\n",
    "\n",
    "workbook = openpyxl.Workbook()\n",
    "\n",
    "source_ws = workbook.active\n",
    "source_ws.title = 'source_df'\n",
    "\n",
    "\n",
    "rows = dataframe_to_rows(source_df, index=False)\n",
    "for r_idx, row in enumerate(rows, 1):\n",
    "    for c_idx, value in enumerate(row, 1):\n",
    "        source_ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "        \n",
    "        \n",
    "summary_ws = workbook.create_sheet('summary')\n",
    "# Construct test worksheets with some content.\n",
    "rows = [\n",
    "    [\"Missing Source Column\", value_to_excel(quality_check_obj[\"missing_columns_source\"]) ],\n",
    "    [\"Primary Key Duplicated\", value_to_excel(quality_check_obj[\"is_pkduplicated\"])],\n",
    "    [\"Primary Has Null Values\", value_to_excel(quality_check_obj[\"is_pknull\"])],\n",
    "    [\"Primary Key Details\"]\n",
    "    ]\n",
    "\n",
    "for col, values in quality_check_obj[\"primary_key_dict\"].items():\n",
    "    v = str(values).replace(\"[]\",\"\")\n",
    "    # print(col)\n",
    "    # print(v)\n",
    "    rows.append([col, v])\n",
    "    \n",
    "for col, values in quality_check_obj[\"datatype_missmatch\"].items():\n",
    "    v = str(values).replace(\"[]\",\"\")\n",
    "    rows.append([col, v])\n",
    "# print(rows)\n",
    "for row in rows:\n",
    "    # print(row)\n",
    "    summary_ws.append(row)\n",
    "    \n",
    "workbook.save('quality_checks.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_quality_checks_xlxs_ons3(source_df, quality_check_obj):    \n",
    "    \n",
    "    pd.options.mode.chained_assignment = None\n",
    "    \n",
    "    copy_source = source_df\n",
    "    missing_columns = quality_check_obj.missing_columns_source\n",
    "    \n",
    "    if not quality_checks_dict[\"MissingColumns\"][\"discard\"]: #include\n",
    "        for col in missing_columns:\n",
    "            copy_source[col +(\"(missing)\")] = \"\"\n",
    "    else: # dicard columns\n",
    "        copy_source.drop(missing_columns)\n",
    "\n",
    "    copy_source[\"check_fail_errors\"] = \"\"\n",
    "    copy_source[\"discard_flag\"] = 0 # 0 -keep 1-discard\n",
    "    \n",
    "        \n",
    "    ## adding primarykey and duplicated missing check_fail_errors...    \n",
    "    if quality_check_obj.is_pkduplicated and quality_check_obj.is_pknull: \n",
    "        pk_dict = quality_check_obj.primary_key_dict           \n",
    "        for col in pk_dict:            \n",
    "            if len(pk_dict[col][\"null\"]) > 0 : # index of null values\n",
    "                for i in pk_dict[col][\"null\"]:\n",
    "                    copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \": Primary key is Null ]\"\n",
    "                    \n",
    "            if len(pk_dict[col][\"duplicated\"]) > 0 : # index of duplicated value\n",
    "                for i in pk_dict[col][\"duplicated\"]: # i is cell_index\n",
    "                    copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \": Primary key is duplicated ]\"\n",
    "    \n",
    "    ## adding datatype missmatch check_fail_errors\n",
    "    dt_dict = pydantic_to_dict(quality_check_obj.datatype_missmatch)\n",
    "    \n",
    "    set2 = set(i for i in range(0, source_df.shape[0])) # all rows index\n",
    "    \n",
    "    for col in dt_dict:\n",
    "        if not dt_dict[col][\"both_same\"]: # if source and report columns not same\n",
    "            \n",
    "            datatype_obj = DataTypes( **dt_dict[col][\"datatypes\"])\n",
    "            \n",
    "            if len(datatype_obj.null_index): # any datatype null should be highlighted\n",
    "                for i in datatype_obj.null_index:\n",
    "                    copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" Null value ]\"\n",
    "            \n",
    "            if not datatype_obj.all_same_datatype: # if not same than hightlight it\n",
    "                \n",
    "                if datatype_obj.identified_data_type == \"integer\": # majority int found                    \n",
    "                    \n",
    "                    int_details = datatype_obj.integer_details\n",
    "                    # all integer include zero and -ve\n",
    "                    set_int = set(int_details.int_index) #int list which not need to hightlight\n",
    "                    not_int_set = set2 - set_int # index which hv not int\n",
    "                    for i in not_int_set:\n",
    "                        copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" not Integer ]\"\n",
    "                    \n",
    "                    # all integer not zero\n",
    "                    set_zero = set(int_details.int_zero_index) #int list which is 0\n",
    "                    for i in set_zero:\n",
    "                        copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" is Zero ]\"\n",
    "                        \n",
    "                    # all integer not -ve\n",
    "                    set_negative = set(int_details.int_nagative_index) #int list which not need to hightlight\n",
    "                    for i in set_negative:\n",
    "                        copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" is Negative ]\"\n",
    "                        \n",
    "                elif datatype_obj.identified_data_type == \"string\": # majority string found\n",
    "                    set_string = set(datatype_obj.string_index) #string list which not need to hightlight\n",
    "                    not_string_set = set2 - set_string # index which hv not string..\n",
    "                    \n",
    "                    for i in not_string_set:\n",
    "                        copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" not String ]\"\n",
    "                        \n",
    "                elif \"decimal\" in datatype_obj.identified_data_type:# majority decimal found\n",
    "                    decimal_detail = datatype_obj.decimal_details\n",
    "                    set_decimal = set(decimal_detail.decimal_index) #decimal list which not need to hightlight\n",
    "                    diff = set2 - set_decimal # index which hv not decimal\n",
    "                    for i in diff:\n",
    "                        copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" not decimal with \" + decimal_detail.decimal_format + \" format]\"\n",
    "                \n",
    "                elif \"Date\" in datatype_obj.identified_data_type:\n",
    "                    date_detail = datatype_obj.date_details                    \n",
    "                    set_date = set(date_detail.date_index)\n",
    "                    \n",
    "                    diff = set2 - set_date # index which hv not decimal\n",
    "                    for i in diff:\n",
    "                        copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" not a date format with \" + str(date_detail.date_format) + \" format]\"                             \n",
    "                             \n",
    "    print(\"*********** new excel witch check file records *******\")  \n",
    "    print(copy_source.head(2))\n",
    "    \n",
    "    copy_source.to_csv('sample.csv')\n",
    "    \n",
    "    \n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_quality_checks_xlxs_ons3(source_df, quality_check_obj):    \n",
    "    \n",
    "    def value_to_excel(v):\n",
    "        return str(v).replace(\"[]\",\"\")\n",
    "\n",
    "    workbook = openpyxl.Workbook()\n",
    "    source_ws = workbook.active\n",
    "    source_ws.title = 'source'\n",
    "    summary_ws = workbook.create_sheet('summary')\n",
    "    \n",
    "\n",
    "    ## creating extra columns\n",
    "    copy_source = source_df\n",
    "    missing_columns = quality_check_obj.missing_columns_source\n",
    "    \n",
    "    if not quality_checks_dict[\"MissingColumns\"][\"discard\"]: #include\n",
    "        for col in missing_columns:\n",
    "            copy_source[col +(\"(missing)\")] = \"\"\n",
    "    else: # dicard columns\n",
    "        copy_source.drop(missing_columns)\n",
    "\n",
    "    copy_source[\"check_fail_errors\"] = \"\"\n",
    "    copy_source[\"discard_flag\"] = 0 # 0 -keep 1-discard\n",
    "    \n",
    "    summary_rows = [\n",
    "    [\"Missing Source Column\", value_to_excel(quality_check_obj[\"missing_columns_source\"]) ],\n",
    "    [\"Primary Key Duplicated\", value_to_excel(quality_check_obj[\"is_pkduplicated\"])],\n",
    "    [\"Primary Has Null Values\", value_to_excel(quality_check_obj[\"is_pknull\"])]\n",
    "    ]\n",
    "            \n",
    "    ## adding primarykey and duplicated missing check_fail_errors...    \n",
    "    if quality_check_obj.is_pkduplicated and quality_check_obj.is_pknull: \n",
    "        pk_dict = quality_check_obj.primary_key_dict\n",
    "                  \n",
    "        for col in pk_dict:\n",
    "            if len(pk_dict[col][\"null\"]) > 0: #if index is not null\n",
    "                summary_rows.append([\"Number of Null in Primary Keys\"])             \n",
    "                summary_rows.append([col, len(pk_dict[col][\"null\"])]) # colname: len - for xl summary                 \n",
    "                    \n",
    "                for i in pk_dict[col][\"null\"]:\n",
    "                    if not quality_checks_dict[\"PrimaryKeyDuplicates\"][\"discard\"]:# not discard, notify...\n",
    "                        copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \": Primary key is Null ]\"\n",
    "                    else:\n",
    "                        copy_source[\"discard_flag\"][i] = 1\n",
    "\n",
    "            if len(pk_dict[col][\"duplicated\"]) > 0: # index of duplicated value                 \n",
    "                summary_rows.append([\"Number of Duplicated in Primary Keys\"])             \n",
    "                summary_rows.append([col, len(pk_dict[col][\"duplicated\"])]) # colname: len - for xl summary\n",
    "                \n",
    "                for i in pk_dict[col][\"duplicated\"]: # i is cell_index\n",
    "                    if not quality_checks_dict[\"PrimaryKeyDuplicates\"][\"discard\"]:#not discard, notify\n",
    "                        copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \": Primary key is duplicated ]\"\n",
    "                    else:\n",
    "                        copy_source[\"discard_flag\"][i] = 1\n",
    "    \n",
    "    ## adding datatype missmatch check_fail_errors\n",
    "    dt_dict = pydantic_to_dict(quality_check_obj.datatype_missmatch)\n",
    "    \n",
    "    set2 = set(i for i in range(0, source_df.shape[0])) # all rows index\n",
    "    \n",
    "    for col in dt_dict:\n",
    "        if not dt_dict[col][\"both_same\"]: # if source and report columns not same\n",
    "            \n",
    "            datatype_obj = DataTypes( **dt_dict[col][\"datatypes\"])\n",
    "            \n",
    "            if len(datatype_obj.null_index) > 0: # any datatype null should be highlighted\n",
    "                for i in datatype_obj.null_index:\n",
    "                    if not quality_checks_dict[\"DataTypeMismatch\"][\"discard\"]: # if not discard, notify\n",
    "                        copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" Null value ]\"\n",
    "                    else:\n",
    "                        copy_source[\"discard_flag\"][i] = 1\n",
    "            \n",
    "            if not datatype_obj.all_same_datatype: # if not same than hightlight it\n",
    "                \n",
    "                if datatype_obj.identified_data_type == \"integer\": # majority int found                    \n",
    "                    \n",
    "                    int_details = datatype_obj.integer_details\n",
    "                    # all integer include zero and -ve\n",
    "                    set_int = set(int_details.int_index) #int list which not need to hightlight\n",
    "                    not_int_set = set2 - set_int # index which hv not int\n",
    "                    for i in not_int_set:\n",
    "                        if not quality_checks_dict[\"DataTypeMismatch\"][\"discard\"]: # if not discard, notify\n",
    "                            copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" not Integer ]\"\n",
    "                        else:\n",
    "                            copy_source[\"discard_flag\"][i] = 1\n",
    "                            \n",
    "                    # all integer not zero\n",
    "                    set_zero = set(int_details.int_zero_index) #int list which is 0\n",
    "                    for i in set_zero:\n",
    "                        if not quality_checks_dict[\"DataTypeMismatch\"][\"discard\"]: # if not discard, notify\n",
    "                            copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" is Zero ]\"\n",
    "                        else:\n",
    "                            copy_source[\"discard_flag\"][i] = 1\n",
    "                        \n",
    "                    # all integer not -ve\n",
    "                    set_negative = set(int_details.int_nagative_index) #int list which not need to hightlight\n",
    "                    for i in set_negative:\n",
    "                        if not quality_checks_dict[\"DataTypeMismatch\"][\"discard\"]: #if not discard, notify\n",
    "                            copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" is Negative ]\"\n",
    "                        else:\n",
    "                            copy_source[\"discard_flag\"][i] = 1\n",
    "                            \n",
    "                elif datatype_obj.identified_data_type == \"string\": # majority string found\n",
    "                    set_string = set(datatype_obj.string_index) #string list which not need to hightlight\n",
    "                    not_string_set = set2 - set_string # index which hv not string..\n",
    "                    \n",
    "                    for i in not_string_set:\n",
    "                        if not quality_checks_dict[\"DataTypeMismatch\"][\"discard\"]: \n",
    "                            copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" not String ]\"\n",
    "                        else:\n",
    "                            copy_source[\"discard_flag\"][i] = 1\n",
    "                            \n",
    "                elif \"decimal\" in datatype_obj.identified_data_type:# majority decimal found\n",
    "                    decimal_detail = datatype_obj.decimal_details\n",
    "                    set_decimal = set(decimal_detail.decimal_index) #decimal list which not need to hightlight\n",
    "                    diff = set2 - set_decimal # index which hv not decimal\n",
    "                    for i in diff:\n",
    "                        if not quality_checks_dict[\"DataTypeMismatch\"][\"discard\"]:\n",
    "                            copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" not decimal with \" + decimal_detail.decimal_format + \" format]\"\n",
    "                        else:\n",
    "                            copy_source[\"discard_flag\"][i] = 1\n",
    "                            \n",
    "                elif \"Date\" in datatype_obj.identified_data_type:\n",
    "                    date_detail = datatype_obj.date_details                    \n",
    "                    set_date = set(date_detail.date_index)\n",
    "                    \n",
    "                    diff = set2 - set_date # index which hv not decimal\n",
    "                    for i in diff:\n",
    "                        if not quality_checks_dict[\"DataTypeMismatch\"][\"discard\"]:\n",
    "                            copy_source[\"check_fail_errors\"][i] = copy_source[\"check_fail_errors\"][i] + \"[\" + col + \" not a date format with \" + str(date_detail.date_format) + \" format]\"                             \n",
    "                        else:\n",
    "                            copy_source[\"discard_flag\"][i] = 1\n",
    "    \n",
    "    ## discard data having flag 1\n",
    "    copy_source = copy_source.loc[copy_source[\"discard_flag\"] == 0]\n",
    "    \n",
    "    ## write source dataframes into xlxs file\n",
    "    df_rows = dataframe_to_rows(copy_source, index=False)\n",
    "    for r_idx, row in enumerate(df_rows, 1):\n",
    "        for c_idx, value in enumerate(row, 1):\n",
    "            source_ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "            \n",
    "    for row in summary_rows:\n",
    "        summary_ws.append(row)\n",
    "    \n",
    "    workbook.save('quality_checks_summary.xlsx')                              \n",
    "                            \n",
    "    print(\"*********** created quality checks with summary.. *******\")  \n",
    "    \n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '178t'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m int_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m178t\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '178t'"
     ]
    }
   ],
   "source": [
    "\n",
    "int_value = int(\"178t\")\n",
    "# str(int_value) == s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dd-mm-yyyy hh:mm\n"
     ]
    }
   ],
   "source": [
    "s = 'Date (dd-mm-yyyy hh:mm)'\n",
    "s = s.replace(\"Date (\",\"\").replace(\")\",\"\")\n",
    "\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 3, 1, 1, 0)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "s = '01-03-2021 1:00'\n",
    "datetime.strptime(s, '%d-%m-%Y %H:%M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not(\"decimal\" in \"decimal(10,2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "ff = float(\"3\")\n",
    "\n",
    "int_f = int(ff)\n",
    "print(str(int_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(int_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data_type_v2(sr):\n",
    "    def is_none_nan_or_blank(var):\n",
    "        if var is None:\n",
    "            return True\n",
    "\n",
    "        if isinstance(var, float) and math.isnan(var):\n",
    "            return True\n",
    "\n",
    "        if isinstance(var, str) and var.strip() == '':\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_date_format(date_string):\n",
    "        format_list = []\n",
    "        for dt_format in format_dict:\n",
    "            try:\n",
    "                datetime.strptime(date_string, format_dict[dt_format])\n",
    "                format_list.append(\"Date (\" + dt_format + \")\")\n",
    "            except ValueError:\n",
    "                pass\n",
    "        return format_list\n",
    "\n",
    "    def is_integer(s):\n",
    "        try:\n",
    "            int_value = int(s)\n",
    "            return str(int_value) == s\n",
    "        except ValueError:\n",
    "            return False\n",
    "    \n",
    "    v_decimal = []\n",
    "    v_int = []\n",
    "    v_int_nagative = []\n",
    "    v_int_zero = []\n",
    "    v_string = []\n",
    "    v_date = dict()\n",
    "    v_none = []\n",
    "    max_after_decimal = 0\n",
    "    max_before_decimal = 0\n",
    "    date_format_list = []\n",
    "\n",
    "    for i, v in sr.items():\n",
    "        if is_none_nan_or_blank(v):\n",
    "            v_none.append(i)\n",
    "            continue\n",
    "\n",
    "        if isinstance(v, np.generic):\n",
    "            v = v.item()\n",
    "\n",
    "        if is_integer(v):\n",
    "            v_int.append(i)\n",
    "            if int(v) == 0:\n",
    "                v_int_zero.append(i)\n",
    "            elif int(v) < 0:\n",
    "                v_int_nagative.append(i)\n",
    "        else:\n",
    "            try:\n",
    "                float(v)\n",
    "                v_decimal.append(i)\n",
    "            except ValueError:\n",
    "                date_format = get_date_format(v)\n",
    "                if date_format:\n",
    "                    if i == 0:\n",
    "                        v_date.update({i:date_format})\n",
    "                        date_format_list.extend(date_format)\n",
    "                    else:\n",
    "                        for dt in date_format:\n",
    "                            v_date.update({i:date_format})\n",
    "                            if dt in date_format_list:\n",
    "                                continue\n",
    "                            else:\n",
    "                                date_format_list.extend(date_format)\n",
    "                        \n",
    "                else:\n",
    "                    v_string.append(i)\n",
    "\n",
    "    v_decimal_len = \"\"\n",
    "    v_decimal_format = \"\"\n",
    "    if len(v_decimal) > 0:\n",
    "        for i, v in sr.items():\n",
    "            if \".\" in str(v):\n",
    "                v = str(v).lstrip(\"-\")\n",
    "                max_after_decimal = max(max_after_decimal, len(v.split(\".\")[1]))\n",
    "                max_before_decimal = max(max_before_decimal, len(v.split(\".\")[0]))\n",
    "        max_after_decimal = min(10, max_after_decimal)\n",
    "        v_decimal_format = f\"decimal({str(max_before_decimal + max_after_decimal)},{str(max_after_decimal)})\"\n",
    "\n",
    "    ##deciding final datatype\n",
    "    v_data_type = \"\"\n",
    "    all_same_datatype = False\n",
    "\n",
    "    if (len(v_int) > len(v_decimal) and len(v_int) > len(v_string) and len(v_int) > len(v_date)):\n",
    "        v_data_type = \"integer\"\n",
    "    elif len(v_decimal) > len(v_int) and len(v_decimal) > len(v_string) and len(v_decimal) > len(v_date):\n",
    "        v_data_type = v_decimal_format\n",
    "    elif len(v_date) > len(v_string):\n",
    "        if len(date_format_list) == 1: # getting only one format\n",
    "            v_data_type = \"Date (\" + str(date_format_list) + \")\"\n",
    "        else:\n",
    "            v_data_type = \"Date\"\n",
    "    else:\n",
    "        v_data_type = \"string\"\n",
    "    \n",
    "    if ((len(v_decimal) == len(sr)) or \n",
    "        (len(v_int) == len(sr)) or\n",
    "        (len(v_date) == len(sr)) or\n",
    "        len(v_string) == len(sr)):\n",
    "        all_same_datatype = True\n",
    "    \n",
    "    # return {\"identified_data_type\": v_data_type,\n",
    "    #         \"all_same_datatype\": all_same_datatype, \n",
    "    #         \"null_index\": v_none,\n",
    "    #         \"integer_index\": {\"int_index\": v_int, \n",
    "    #                         \"int_nagative_index\": v_int_nagative,\n",
    "    #                         \"int_zero_index\":v_int_zero},\n",
    "    #         \"string_index\": v_string,\n",
    "    #         \"decimal_details\": {\"decimal_index\": v_decimal, \n",
    "    #                             \"decimal_format\": v_decimal_format },\n",
    "    #         \"date_details\": {\"date_index\":v_date,\n",
    "    #                          \"date_format\":date_format_list}            \n",
    "    #         }\n",
    "    \n",
    "    integer_details = IntegerDetails( int_index = v_int,\n",
    "                                     int_nagative_index = v_int_nagative,\n",
    "                                     int_zero_index = v_int_zero)    \n",
    "    decimal_details = DecimalDetails( decimal_index = v_decimal,\n",
    "                                     decimal_format = v_decimal_format)    \n",
    "    date_details = DateDetails( date_index = v_date,\n",
    "                               date_format = date_format_list ) \n",
    "    datatype_details = DataTypes( identified_data_type = v_data_type,\n",
    "                                 all_same_datatype = all_same_datatype,\n",
    "                                 null_index= v_none,\n",
    "                                 integer_details = integer_details,\n",
    "                                 string_index = v_string,\n",
    "                                 decimal_details = decimal_details,\n",
    "                                 date_details = date_details)\n",
    "    return datatype_details\n",
    "    \n",
    "    \n",
    "def check_datatype_mismatch_v2(source_df, report_columns): #report_columns is b_source_column object list\n",
    "    \n",
    "    datatype_dict = {obj.col_name_user_defined : obj.data_type for obj in report_columns}\n",
    "    \n",
    "    identified_datatype_dict = dict()\n",
    "    for col in source_df.columns:\n",
    "        identified_datatype_dict.update({col:{\"datatype_details\": get_data_type(source_df[col])}}) \n",
    "               \n",
    "        source_df_col_dt = identified_datatype_dict[col][\"datatype_details\"] ## converting to pydentic model\n",
    "        \n",
    "        if(source_df_col_dt.all_same_datatype):\n",
    "            \n",
    "            if  ((\"Date\" in source_df_col_dt.identified_data_type) and (\"Date\" in datatype_dict[col]) ) or \\\n",
    "            ((\"decimal\" in source_df_col_dt.identified_data_type) and (\"decimal\" in datatype_dict[col])) or \\\n",
    "            (source_df_col_dt.identified_data_type == datatype_dict[col]):    \n",
    "                            \n",
    "                identified_datatype_dict.update({col: DataTypesMissmatch(datatypes = source_df_col_dt, both_same= True)})\n",
    "        else:\n",
    "            identified_datatype_dict.update({col: DataTypesMissmatch(datatypes = source_df_col_dt, both_same= False)})\n",
    "        \n",
    "    return identified_datatype_dict\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
